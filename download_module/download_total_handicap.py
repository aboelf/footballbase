#!/usr/bin/env python3
"""
æ‰¹é‡ä¸‹è½½æ€»äºšç›˜æ•°æ®è„šæœ¬

åŠŸèƒ½ï¼š
1. ä» Supabase league_matches è¡¨è·å–æ‰€æœ‰ match_id
2. å¤ç”¨ Playwright æµè§ˆå™¨æ‰¹é‡ä¸‹è½½æ€»äºšç›˜æ•°æ®
3. ä¿å­˜åˆ° rawdata/odds/total_handicap ç›®å½•

ç”¨æ³•ï¼š
    python download_total_handicap.py [--limit N] [--match-id ID] [--dry-run]
"""

import os
import sys
import json
import time
import argparse
from datetime import datetime
from dotenv import load_dotenv

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from downloader import DataDownloader


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def get_supabase_client():
    """è·å– Supabase REST å®¢æˆ·ç«¯"""
    from postgrest import SyncPostgrestClient

    url = os.getenv("SUPABASE_URL", "").rstrip("/")
    key = os.getenv("SUPABASE_KEY", "")

    if not url or not key:
        raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® SUPABASE_URL å’Œ SUPABASE_KEY")

    return SyncPostgrestClient(
        f"{url}/rest/v1",
        headers={
            "Authorization": f"Bearer {key}",
            "apikey": key,
            "Content-Type": "application/json",
        },
    )


def fetch_all_match_ids(client) -> list[int]:
    """ä» Supabase è·å–æ‰€æœ‰æ¯”èµ› ID"""
    print("ä» Supabase è·å– league_matches æ•°æ®...")

    try:
        # è·å–æ‰€æœ‰ match_id
        result = client.table("league_matches").select("match_id").execute()

        if not result.data:
            print("  è­¦å‘Š: Supabase ä¸­æ²¡æœ‰æ‰¾åˆ°æ¯”èµ›æ•°æ®")
            return []

        match_ids = [item["match_id"] for item in result.data]
        print(f"  å·²åŠ è½½ {len(match_ids)} ä¸ªæ¯”èµ› ID")
        return match_ids

    except Exception as e:
        print(f"  è·å–æ•°æ®å¤±è´¥: {e}")
        return []


def download_total_handicap_batch(
    downloader: DataDownloader,
    match_ids: list[int],
    output_dir: str = "./rawdata/odds/total_handicap",
    limit: int | None = None,
    dry_run: bool = False,
    delay: float = 0.5,
):
    """
    æ‰¹é‡ä¸‹è½½æ€»äºšç›˜æ•°æ®ï¼ˆå¤ç”¨æµè§ˆå™¨ï¼‰

    Args:
        downloader: DataDownloader å®ä¾‹
        match_ids: è¦ä¸‹è½½çš„æ¯”èµ› ID åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        limit: æœ€å¤šä¸‹è½½æ•°é‡ (None è¡¨ç¤ºå…¨éƒ¨)
        dry_run: è¯•è¿è¡Œæ¨¡å¼ï¼Œåªæ‰“å°ä¸ä¸‹è½½
        delay: ä¸‹è½½é—´éš”ï¼ˆç§’ï¼‰
    """
    if limit:
        match_ids = match_ids[:limit]

    total = len(match_ids)
    print(f"\nå‡†å¤‡ä¸‹è½½ {total} åœºæ¯”èµ›çš„æ€»äºšç›˜æ•°æ®...")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  è¯•è¿è¡Œ: {'æ˜¯' if dry_run else 'å¦'}")
    print(f"  ä¸‹è½½é—´éš”: {delay} ç§’")
    print()

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not dry_run:
        os.makedirs(output_dir, exist_ok=True)

    # æ›´æ–° downloader çš„ base_path
    original_base_path = downloader.base_path
    downloader.base_path = output_dir

    success_count = 0
    fail_count = 0
    skipped_count = 0

    # å¤ç”¨æµè§ˆå™¨
    browser = None
    page = None

    try:
        from playwright.sync_api import sync_playwright

        if not dry_run:
            print("ğŸš€ å¯åŠ¨ Playwright æµè§ˆå™¨...")
            pw = sync_playwright().start()
            browser = pw.chromium.launch(headless=True)
            page = browser.new_page()
            print("âœ… æµè§ˆå™¨å·²å¯åŠ¨ï¼Œå°†å¤ç”¨æµè§ˆå™¨æ‰¹é‡ä¸‹è½½\n")

        for i, match_id in enumerate(match_ids, 1):
            print(f"[{i}/{total}] æ¯”èµ› ID: {match_id}", end=" ")

            if dry_run:
                print("â­ è·³è¿‡ (è¯•è¿è¡Œæ¨¡å¼)")
                continue

            try:
                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½è¿‡
                filename = f"{output_dir}/{match_id}_total.html"
                if os.path.exists(filename):
                    print(f"â­ å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    skipped_count += 1
                    continue

                # ä½¿ç”¨å¤ç”¨æµè§ˆå™¨ä¸‹è½½
                url = f"https://vip.titan007.com/AsianOdds_n.aspx?id={match_id}&l=0"

                page.goto(url, wait_until="networkidle")
                page.wait_for_timeout(1000)  # ç­‰å¾…åŠ¨æ€å†…å®¹
                html_content = page.content()

                if html_content:
                    # ä½¿ç”¨ GBK ç¼–ç ä¿å­˜ï¼ˆtitan007.com ä½¿ç”¨ä¸­æ–‡ç¼–ç ï¼‰
                    with open(filename, "w", encoding="gbk", errors="replace") as f:
                        f.write(html_content)

                    print("âœ“ æˆåŠŸ")
                    success_count += 1
                else:
                    print("âœ— å¤±è´¥: æ— å†…å®¹")
                    fail_count += 1

            except Exception as e:
                print(f"âœ— é”™è¯¯: {e}")
                fail_count += 1

            # ç­‰å¾…é—´éš”
            if i < total:
                time.sleep(delay)

    except ImportError:
        print("âš  Playwright æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨ä¸‹è½½æ€»äºšç›˜æ•°æ®")
        print("  è¯·å®‰è£… Playwright: pip install playwright && playwright install")
    finally:
        # å…³é—­æµè§ˆå™¨
        if browser:
            browser.close()
            print("\nğŸ”’ æµè§ˆå™¨å·²å…³é—­")

    # æ¢å¤åŸå§‹è®¾ç½®
    downloader.base_path = original_base_path

    # æ‰“å°ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ä¸‹è½½å®Œæˆ!")
    print("=" * 60)
    print(f"  æ€»æ•°: {total}")
    print(f"  æˆåŠŸ: {success_count}")
    print(f"  å¤±è´¥: {fail_count}")
    print(f"  è·³è¿‡: {skipped_count}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")

    return {
        "total": total,
        "success": success_count,
        "failed": fail_count,
        "skipped": skipped_count,
    }


def main():
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡ä¸‹è½½æ€»äºšç›˜æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # ä¸‹è½½æ‰€æœ‰æ¯”èµ›çš„æ€»äºšç›˜æ•°æ®
    python download_total_handicap.py

    # åªä¸‹è½½å‰ 10 ä¸ª
    python download_total_handicap.py --limit 10

    # åªä¸‹è½½æŒ‡å®šæ¯”èµ›
    python download_total_handicap.py --match-id 2799893

    # è¯•è¿è¡Œï¼ˆä¸å®é™…ä¸‹è½½ï¼‰
    python download_total_handicap.py --dry-run
        """,
    )

    parser.add_argument(
        "--limit", type=int, default=None, help="æœ€å¤šä¸‹è½½æ•°é‡ (é»˜è®¤: å…¨éƒ¨)"
    )
    parser.add_argument(
        "--match-id",
        type=int,
        default=None,
        help="åªä¸‹è½½æŒ‡å®šæ¯”èµ› ID (ä¸ --limit äº’æ–¥)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./rawdata/odds/total_handicap",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./rawdata/odds/total_handicap)",
    )
    parser.add_argument(
        "--delay", type=float, default=0.3, help="ä¸‹è½½é—´éš”ç§’æ•° (é»˜è®¤: 0.3)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="è¯•è¿è¡Œæ¨¡å¼ï¼Œåªæ‰“å°ä¸ä¸‹è½½",
    )

    args = parser.parse_args()

    print("=" * 60)
    print("æ‰¹é‡ä¸‹è½½æ€»äºšç›˜æ•°æ®")
    print("=" * 60)

    # åˆ›å»ºä¸‹è½½å™¨
    downloader = DataDownloader(base_path=args.output_dir)

    # è·å–æ¯”èµ› ID
    if args.match_id:
        match_ids = [args.match_id]
        print(f"ä½¿ç”¨æŒ‡å®šçš„æ¯”èµ› ID: {args.match_id}")
    else:
        try:
            client = get_supabase_client()
            match_ids = fetch_all_match_ids(client)
        except ValueError as e:
            print(f"\né…ç½®é”™è¯¯: {e}")
            print("è¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­å·²è®¾ç½® SUPABASE_URL å’Œ SUPABASE_KEY")
            sys.exit(1)
        except Exception as e:
            print(f"\nè¿æ¥ Supabase å¤±è´¥: {e}")
            sys.exit(1)

    if not match_ids:
        print("æ²¡æœ‰æ‰¾åˆ°å¯ä¸‹è½½çš„æ¯”èµ› ID")
        sys.exit(0)

    # ä¸‹è½½æ€»äºšç›˜æ•°æ®ï¼ˆå¤ç”¨æµè§ˆå™¨ï¼‰
    result = download_total_handicap_batch(
        downloader=downloader,
        match_ids=match_ids,
        output_dir=args.output_dir,
        limit=args.limit,
        dry_run=args.dry_run,
        delay=args.delay,
    )

    # ä¿å­˜ä¸‹è½½è®°å½•
    if not args.dry_run:
        log_file = os.path.join(args.output_dir, "download_log.json")
        result["download_time"] = datetime.now().isoformat()
        result["output_dir"] = args.output_dir

        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nä¸‹è½½è®°å½•å·²ä¿å­˜: {log_file}")


if __name__ == "__main__":
    main()
